# Precision-Focused Configuration for Japanese Grammar Error Detection
# Strategy: Quality over quantity, precise detection patterns

model: "mlx-community/Qwen3-0.6B-4bit"
data_dir: "datasets/gec_error_detection"  # Directory containing train.jsonl, valid.jsonl, test.jsonl
adapter_path: "models/japanese-gec-detect-v4"

# Balanced LoRA/DoRA parameters for stable learning
num_layers: 16  # Number of layers to fine-tune
lora_parameters:
  rank: 16  # LoRA rank for good representation without overfitting
  scale: 16  # Reduced scale for more stable training
  dropout: 0.05  # Lower dropout for better learning stability

# Stable training parameters for consistent performance
batch_size: 8  # Larger batch for more stable gradients
learning_rate: 1e-5  # Lower learning rate for stable convergence
iters: 4000  # Extended training with stable learning
val_batches: 50  # Regular validation monitoring
steps_per_report: 20
steps_per_eval: 100  # Frequent evaluation for early stopping
steps_per_save: 200

# Generation parameters optimized for detection precision
max_tokens: 300  # Sufficient length for detection patterns
temperature: 0.01  # Extremely low temperature for deterministic output
repetition_penalty: 1.0  # No penalty to avoid interfering with detection patterns

# Precision-focused evaluation settings
eval_steps: 100
save_steps: 200
logging_steps: 20

# Optimized data processing for detection task
max_seq_length: 512  # Balanced context length for most sentences
truncation: true
padding: true

# Detection-optimized settings
fine_tune_type: "dora"  # LoRA for parameter efficiency
mask_prompt: true  # Focus learning on detection outputs only
grad_checkpoint: false  # Disable for faster training with smaller config

# Stable optimizer configuration
optimizer: "adamw"  # AdamW supports weight decay for regularization
optimizer_config:
  adamw:
    weight_decay: 0.01  # Standard weight decay for regularization

# Simplified learning rate schedule for stability
# lr_schedule:
#   name: "cosine_decay"
#   arguments: [1e-5, 4000]  # [lower_lr, matching_steps]
#   warmup: 200  # Shorter warmup for faster convergence
#   warmup_init: 1e-6  # Gradual start

# Detection-specific optimizations
seed: 42  # Fixed seed for reproducible results