# Training Guide for Japanese Grammar Correction

This guide provides detailed instructions for training the Japanese Grammar Correction model using the MLX LoRA fine-tuning approach.

## Overview

The training process consists of three main phases:
1. **Data Preprocessing**: Convert Japanese GEC corpus to MLX-compatible format
2. **Model Fine-tuning**: Train LoRA adapters on the Qwen3-0.6B-4bit model
3. **Evaluation**: Assess model performance on test data

## Prerequisites

- Apple Silicon Mac (M1/M2/M3)
- Python 3.8+
- MLX framework installed
- Japanese GEC corpus data
- At least 8GB RAM and 10GB free disk space

## Step-by-Step Training Process

### 1. Data Preparation

#### Corpus Format Requirements

Your Japanese GEC corpus should be in tab-separated format:

```
ID	Marked_Text	Error_Type
1	私は<昨日>（昨日）映画を見ました。	Correct
2	彼は<学校に行く>（学校に行きます）。	Grammar
3	<これは>（これが）私の本です。	Particle
```

**Marker Conventions:**
- `<error_text>`: Original text with grammatical errors
- `(correct_text)`: Grammatically corrected text

#### Data Preprocessing

```bash
# Option 1: Use complete pipeline (recommended)
python scripts/complete_training_pipeline.py --config config/lora_config.yaml

# Option 2: Manual preprocessing
python utils/gec_parser.py \
    --input exclude/japanese_gec_corpus/corpus_v0.txt \
    --output datasets/ \
    --train-ratio 0.8 \
    --valid-ratio 0.1 \
    --test-ratio 0.1
```

### 2. Configuration Setup

#### Basic Configuration (`config/lora_config.yaml`)

```yaml
# Model settings
model: "mlx-community/Qwen3-0.6B-4bit"

# LoRA parameters (adjust based on your needs)
lora_rank: 16          # Higher = more parameters, better quality
lora_alpha: 32         # Scaling factor (usually 2x rank)
lora_dropout: 0.05     # Regularization

# Training parameters
learning_rate: 0.0001  # Conservative for stability
batch_size: 4          # Adjust based on available memory
iters: 2000           # Number of training iterations
val_batches: 50       # Validation frequency

# Data paths (auto-generated by preprocessing)
train: "datasets/train.jsonl"
valid: "datasets/valid.jsonl"
test: "datasets/test.jsonl"

# Output location
adapter_path: "models/japanese-gec-lora"

# Generation settings
max_tokens: 512
temp: 0.1             # Low temperature for deterministic corrections
top_p: 0.9
```

#### Advanced Configuration Options

```yaml
# Memory optimization
grad_checkpoint: true     # Reduce memory usage
max_seq_length: 512      # Maximum sequence length

# Training optimization
warmup_steps: 100        # Learning rate warmup
weight_decay: 0.01       # L2 regularization
seed: 42                 # Reproducible results

# Monitoring
steps_per_report: 25     # Progress reporting frequency
steps_per_eval: 200      # Validation frequency
steps_per_save: 400      # Checkpoint saving frequency
```

### 3. Training Execution

#### Complete Pipeline (Recommended)

```bash
# Run complete training pipeline
python scripts/complete_training_pipeline.py \
    --config config/lora_config.yaml \
    --output-dir training_run_$(date +%Y%m%d_%H%M%S) \
    --log-level INFO
```

#### Manual Training Steps

```bash
# Step 1: Preprocess data (if not done already)
python scripts/complete_training_pipeline.py \
    --skip-training \
    --skip-evaluation \
    --config config/lora_config.yaml

# Step 2: Train model
python scripts/train_japanese_gec.py \
    --config config/lora_config.yaml \
    --log-file logs/training.log

# Step 3: Evaluate model
python scripts/batch_inference.py \
    --model mlx-community/Qwen3-0.6B-4bit \
    --adapter-path models/japanese-gec-lora \
    --input-file datasets/test.jsonl \
    --output-file results/test_predictions.jsonl

python scripts/evaluate_gec.py \
    --results-file results/test_predictions.jsonl \
    --output-file results/evaluation_report.json
```

### 4. Training Monitoring

#### Progress Monitoring

The training script provides real-time progress updates:

```
MLX: Iter 25: Train loss 2.456, Learning Rate 9.50e-05, It/sec 1.23, Tokens/sec 156.7
MLX: Iter 50: Train loss 2.234, Learning Rate 9.00e-05, It/sec 1.25, Tokens/sec 160.2
MLX: Validation loss: 2.123, Validation ppl: 8.345
MLX: Saved checkpoint at iter 100: models/japanese-gec-lora/0000100_adapters.safetensors
```

#### Log Analysis

```bash
# Monitor training progress
tail -f logs/training.log

# Extract loss values
grep "Train loss" logs/training.log | awk '{print $6}' > training_losses.txt

# Extract validation metrics
grep "Validation" logs/training.log > validation_metrics.txt
```

### 5. Hyperparameter Tuning

#### LoRA Parameters

| Parameter | Small Model | Medium Model | Large Model |
|-----------|-------------|--------------|-------------|
| `lora_rank` | 8-16 | 16-32 | 32-64 |
| `lora_alpha` | 16-32 | 32-64 | 64-128 |
| `lora_dropout` | 0.05-0.1 | 0.05-0.1 | 0.1-0.2 |

#### Training Parameters

| Parameter | Fast Training | Balanced | High Quality |
|-----------|---------------|----------|--------------|
| `learning_rate` | 0.0005 | 0.0001 | 0.00005 |
| `batch_size` | 8 | 4 | 2 |
| `iters` | 1000 | 2000 | 5000 |

#### Memory Optimization

For limited memory:
```yaml
batch_size: 1
grad_checkpoint: true
max_seq_length: 256
lora_rank: 8
```

For high memory:
```yaml
batch_size: 8
grad_checkpoint: false
max_seq_length: 1024
lora_rank: 32
```

### 6. Training Strategies

#### Curriculum Learning

Start with simpler examples and gradually increase complexity:

```bash
# Phase 1: Simple corrections (particles, basic grammar)
python scripts/complete_training_pipeline.py \
    --config config/phase1_config.yaml \
    --max-samples 1000

# Phase 2: Complex corrections (tense, honorifics)
python scripts/complete_training_pipeline.py \
    --config config/phase2_config.yaml \
    --skip-preprocessing
```

#### Multi-stage Training

```bash
# Stage 1: High learning rate, short training
python scripts/train_japanese_gec.py \
    --config config/stage1_config.yaml  # lr=0.0005, iters=500

# Stage 2: Lower learning rate, longer training
python scripts/train_japanese_gec.py \
    --config config/stage2_config.yaml  # lr=0.0001, iters=1500
```

### 7. Quality Assessment

#### Training Metrics to Monitor

1. **Training Loss**: Should decrease steadily
2. **Validation Loss**: Should decrease without overfitting
3. **Validation Perplexity**: Lower is better
4. **Learning Rate**: Should follow schedule

#### Early Stopping Criteria

Stop training if:
- Validation loss stops improving for 5+ evaluations
- Training loss becomes much lower than validation loss (overfitting)
- Model starts generating repetitive or nonsensical outputs

### 8. Troubleshooting

#### Common Training Issues

**Issue**: Training loss not decreasing
```yaml
# Solution: Increase learning rate
learning_rate: 0.0005  # from 0.0001
```

**Issue**: Validation loss increasing (overfitting)
```yaml
# Solution: Add regularization
lora_dropout: 0.1      # from 0.05
weight_decay: 0.02     # from 0.01
```

**Issue**: Out of memory errors
```yaml
# Solution: Reduce memory usage
batch_size: 1          # from 4
grad_checkpoint: true
max_seq_length: 256    # from 512
```

**Issue**: Training too slow
```yaml
# Solution: Optimize for speed
batch_size: 8          # if memory allows
steps_per_eval: 500    # from 200
steps_per_save: 1000   # from 400
```

#### Debugging Commands

```bash
# Check GPU memory usage
python -c "import mlx.core as mx; print(mx.metal.get_active_memory() / 1024**3, 'GB')"

# Validate data format
head -5 datasets/train.jsonl | python -m json.tool

# Test model loading
python -c "from mlx_lm import load; load('mlx-community/Qwen3-0.6B-4bit')"
```

### 9. Best Practices

#### Data Quality
- Ensure corpus has diverse error types
- Balance error categories in training data
- Remove duplicate or near-duplicate examples
- Validate corpus encoding (UTF-8)

#### Training Stability
- Use gradient clipping for stability
- Monitor validation metrics closely
- Save checkpoints frequently
- Use reproducible random seeds

#### Resource Management
- Monitor memory usage during training
- Use appropriate batch sizes for your hardware
- Clean up old checkpoints to save disk space

#### Evaluation
- Evaluate on held-out test set
- Use multiple evaluation metrics
- Test on real-world examples
- Compare with baseline models

### 10. Advanced Techniques

#### Data Augmentation

```python
# Example: Add noise to training data
def augment_training_data(pairs):
    augmented = []
    for error, correct in pairs:
        # Original pair
        augmented.append((error, correct))
        
        # Add character-level noise
        noisy_error = add_character_noise(error)
        augmented.append((noisy_error, correct))
    
    return augmented
```

#### Multi-task Learning

Train on multiple related tasks:
- Grammar correction
- Style transfer (casual ↔ polite)
- Sentence completion

#### Transfer Learning

```bash
# Start from a pre-trained adapter
python scripts/train_japanese_gec.py \
    --config config/lora_config.yaml \
    --resume-from models/pretrained-adapters/adapters.safetensors
```

## Conclusion

Successful training of the Japanese Grammar Correction model requires:
1. High-quality, diverse training data
2. Appropriate hyperparameter tuning
3. Careful monitoring and evaluation
4. Iterative improvement based on results

Follow this guide systematically, and adjust parameters based on your specific requirements and available resources.